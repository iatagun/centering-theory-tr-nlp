# Turkish POS & Semantic Analyzer

> **Stanza-based Turkish NLP with POS preferences detection and propositional semantics analysis**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Stanza](https://img.shields.io/badge/stanza-1.5+-green.svg)](https://stanfordnlp.github.io/stanza/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## üéØ Overview

This project provides a comprehensive Turkish NLP analysis toolkit that combines:

- **POS Tagging** with Stanza parser
- **POS Preference Detection** using Minimalist Program theory
- **Propositional Semantics** analysis (analytic vs synthetic)
- **JSON & CONLL-U** structured output formats

### What Makes This Different?

Traditional POS taggers assign universal tags (NOUN, VERB, ADJ) based on syntax. Our system goes deeper:

1. **Detects nominal domain preferences** - Identifies when VERB-origin words prefer nominal behavior
2. **Semantic validation** - Analyzes whether propositions are analytic (generic) or synthetic (time-bound)
3. **Clause finiteness** - Distinguishes finite clauses from embedded non-finite structures
4. **Lexicalization filtering** - Recognizes frozen compounds vs. productive derivations

**Example:**
```
"Ali'nin okuduƒüu kitap burada."
         ‚Üì
"okuduƒüu" (read-DIK-his):
  - Stanza tags: VERB (syntactically correct)
  - Our detection: NOUN preference (90% confidence)
  - Reason: Nominal suffix -DIK ‚Üí partitive predicate ‚Üí specificity ‚Üí nominal domain
```

---

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/centering_test.git
cd centering_test

# Install dependencies
pip install stanza

# Download Turkish model
python -c "import stanza; stanza.download('tr')"

# PyTorch 2.6+ compatibility (if needed)
# Add to your script before importing:
import os
os.environ['TORCH_FORCE_WEIGHTS_ONLY_LOAD'] = '0'
```

### Basic Usage

```python
import os
os.environ['TORCH_FORCE_WEIGHTS_ONLY_LOAD'] = '0'  # PyTorch 2.6+ compatibility

from api.pos_semantic_analyzer import analyze_text
import json

# Analyze a sentence
result = analyze_text("Ku≈ülar u√ßtu.")

# Pretty print JSON
print(json.dumps(result, indent=2, ensure_ascii=False))
```

**Output:**
```json
{
  "text": "Ku≈ülar u√ßtu.",
  "sentences": [{
    "words": [{
      "id": 2,
      "text": "u√ßtu",
      "upos": "VERB",
      "feats": "Aspect=Perf|Tense=Past",
      "is_finite": true,
      "morphology": [],
      "preference": null
    }],
    "semantics": {
      "proposition_type": "synthetic",
      "predicate_type": "partitive",
      "clause_finiteness": "finite",
      "generic_encoding": false,
      "time_bound": true,
      "verifiability": 0.8
    }
  }]
}
```

---

## üìä Features

### 1. POS Preference Detection

Identifies when words show preference for different POS categories:

**Supported Patterns:**
- **-DIK suffix**: VERB ‚Üí NOUN preference (partitive predicate nominalization)
- **-mA suffix**: Productive vs lexicalized distinction
- **-I≈ü suffix**: VERB ‚Üí NOUN (action nominalization)
- **-mAk suffix**: Infinitive forms

**Confidence Levels:**
- `-DIK` with semantic validation: **90-95%**
- `-mA` productive: **80-85%**
- `-mA` lexicalized: **No preference** (filtered out)

### 2. Propositional Semantics

Analyzes propositions using semantic theory:

**Proposition Types:**
- **Analytic**: Generic, always true/false (e.g., "Ku≈ülar u√ßar" - Birds fly)
- **Synthetic**: Time-bound, verifiable (e.g., "Ku≈ülar u√ßtu" - Birds flew)

**Predicate Types:**
- **Holistic** (b√ºt√ºnc√ºl): State/property, no time point (e.g., habitual, copula)
- **Partitive** (par√ßalƒ±): Event, specific time point (e.g., past, future)
- **Habitual** (alƒ±≈ükanlƒ±k): Recurring pattern (e.g., "Ali sabahlarƒ± erken kalkar")

**Clause Finiteness:**
- **Finite**: Independent clause with tensed verb
- **Non-finite**: Embedded clause, copula, or nominal predicate

### 3. Structured Output Formats

#### JSON Format (Stanza-compatible)
Complete linguistic annotation with extensions:
- Word-level: `morphology`, `is_finite`, `preference`
- Sentence-level: `semantics` (proposition analysis)

#### CONLL-U Format
Standard format with preferences in MISC field:
```
# text = Ali'nin okuduƒüu kitap burada.
2  okuduƒüu  oku  VERB  ...  Preference=NOUN|Confidence=0.90|Morphology=-DIK
```

---

## üìÅ Project Structure

```
centering_test/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ pos_semantic_analyzer.py      # üö¢ Main API (flagship)
‚îÇ   ‚îú‚îÄ‚îÄ simple_check.py               # Simple POS preference check
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_analysis.py          # Full semantic integration
‚îÇ   ‚îî‚îÄ‚îÄ main.py                       # Legacy API functions
‚îÇ
‚îú‚îÄ‚îÄ error_detection/
‚îÇ   ‚îî‚îÄ‚îÄ minimalist_pos_error_detection.py  # Minimalist Program detector
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ propositional_semantics.py    # Semantic analysis module
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_comprehensive.py         # Full integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_semantic_integration.py  # Semantic tests
‚îÇ   ‚îú‚îÄ‚îÄ test_minimalist.py           # Minimalist Program tests
‚îÇ   ‚îú‚îÄ‚îÄ test_lexicalized.py          # Lexicalized compound tests
‚îÇ   ‚îî‚îÄ‚îÄ test_pos_fixes.py            # POS fixes validation (17 tests)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ ud_tr_imst/                   # UD Turkish-IMST corpus
‚îÇ
‚îú‚îÄ‚îÄ example_usage.py                  # Usage examples
‚îî‚îÄ‚îÄ README.md                         # This file
```

---

## üî¨ Theoretical Background

### Minimalist Program (Chomsky 1995)

Our POS preference detection is based on Minimalist syntax theory:

**Core Principles:**
1. **Feature Checking**: Nominal suffixes (-DIK, -mA) trigger N-features
2. **Merge Operation**: Builds binary-branching syntactic structures
3. **Movement Theory**: Tracks derivational history (VERB-origin ‚Üí NOUN)

**Key Insight:**
> Morphologically derived nominals retain verbal semantics but show nominal syntactic distribution. This creates a **preference** rather than an error.

### Propositional Semantics

Based on analytic/synthetic proposition distinction:

**Analytic Propositions:**
- Generic reference (bare plurals: "Ku≈ülar" without specificity)
- Holistic predicates (aorist/habitual: "u√ßar")
- Always true/false (100% verifiability)
- Example: "Ku≈ülar u√ßar" (Birds fly - generic property)

**Synthetic Propositions:**
- Specific reference (demonstratives, accusative case)
- Partitive predicates (past, future, progressive)
- Time-bound truth value
- Example: "Ku≈ülar u√ßtu" (Birds flew - specific event)

---

## üí° Usage Examples

### Example 1: Detect POS Preferences

```python
from api.pos_semantic_analyzer import analyze_text

text = "Ali'nin okuduƒüu kitap burada."
result = analyze_text(text)

# Check for preferences
for word in result["sentences"][0]["words"]:
    if word["preference"]:
        print(f"{word['text']}: {word['upos']} ‚Üí {word['preference']['expected_pos']}")
        print(f"  Confidence: {word['preference']['confidence']:.0%}")
        print(f"  Reason: {word['preference']['reason']}")

# Output:
# okuduƒüu: VERB ‚Üí NOUN
#   Confidence: 90%
#   Reason: Nominal suffix detected: ['-DIK']
```

### Example 2: Analyze Semantics

```python
from api.pos_semantic_analyzer import analyze_text

sentences = [
    "Ku≈ülar u√ßar.",           # Analytic
    "Ku≈ülar u√ßtu.",           # Synthetic
    "Ali sabahlarƒ± kalkar."   # Habitual
]

for text in sentences:
    result = analyze_text(text)
    sem = result["sentences"][0]["semantics"]
    
    print(f"{text}")
    print(f"  Type: {sem['proposition_type']}")
    print(f"  Predicate: {sem['predicate_type']}")
    print(f"  Finite: {sem['clause_finiteness']}")
```

### Example 3: CONLL-U Export

```python
from api.pos_semantic_analyzer import analyze_to_conllu

text = "Ali'nin okuduƒüu kitap burada."
conllu = analyze_to_conllu(text)
print(conllu)

# Output:
# # text = Ali'nin okuduƒüu kitap burada.
# 1  Ali'nin  Ali  PROPN  ...
# 2  okuduƒüu  oku  VERB   ...  Preference=NOUN|Confidence=0.90|Morphology=-DIK
# 3  kitap    kitap NOUN  ...
# ...
```

### Example 4: Lexicalized vs Productive

```python
from api.pos_semantic_analyzer import analyze_text

# Lexicalized (no preference)
result1 = analyze_text("Y√ºzme havuzu temiz.")
# "Y√ºzme" ‚Üí No preference (frozen compound)

# Productive (preference detected)
result2 = analyze_text("Yazma defteri aldƒ±m.")
# "Yazma" ‚Üí NOUN preference (85% confidence)
```

---

## üß™ Testing

### Run All Tests

```bash
# Comprehensive integration test
python tests/test_comprehensive.py

# Semantic integration test
python tests/test_semantic_integration.py

# Minimalist Program test
python tests/test_minimalist.py

# Lexicalized compound test
python tests/test_lexicalized.py

# POS fixes validation (all fixes verified)
python tests/test_pos_fixes.py
```

### Test Results

**test_pos_fixes.py**: 17/17 tests passed (100% success) ‚≠ê
- ‚úÖ -DIK suffix nominal preference (3 tests)
- ‚úÖ -mA productive vs lexicalized (2 tests)
- ‚úÖ Generic vs specific propositions (2 tests)
- ‚úÖ Holistic/Partitive/Habitual predicates (3 tests)
- ‚úÖ Finite vs non-finite detection (3 tests)
- ‚úÖ Confidence scoring accuracy (2 tests)
- ‚úÖ English output format (2 tests)

**test_comprehensive.py**: 10/13 tests passed (76.9% success)
- ‚úÖ -DIK detection with 95% confidence
- ‚úÖ Productive -mA detection (80-85%)
- ‚úÖ Lexicalized filtering (no false positives)
- ‚úÖ Generic vs specific distinction
- ‚úÖ Semantic validation boosts confidence

**test_minimalist.py**: All tests passed
- ‚úÖ Propositional semantics available
- ‚úÖ Detection working (1 error found)
- ‚úÖ Lexicalized filtering (0 errors for "Y√ºzme")

---

## üìñ API Reference

### Main Functions

#### `analyze_text(text: str, include_semantics: bool = True) -> Dict`

Analyzes Turkish text with full linguistic annotation.

**Parameters:**
- `text`: Input Turkish text
- `include_semantics`: Include propositional semantics (default: True)

**Returns:**
```python
{
  "text": str,
  "sentences": [
    {
      "text": str,
      "words": [
        {
          "id": int,
          "text": str,
          "lemma": str,
          "upos": str,
          "feats": str,
          "morphology": List[str],
          "is_finite": bool,
          "preference": {
            "type": str,
            "expected_pos": str,
            "confidence": float,
            "reason": str
          } | None
        }
      ],
      "semantics": {
        "proposition_type": "analytic" | "synthetic",
        "predicate_type": "holistic" | "partitive" | "habitual",
        "generic_encoding": bool,
        "time_bound": bool,
        "verifiability": float,
        "clause_finiteness": "finite" | "non-finite"
      } | None
    }
  ]
}
```

#### `analyze_to_conllu(text: str) -> str`

Exports analysis to CONLL-U format.

**Parameters:**
- `text`: Input Turkish text

**Returns:** CONLL-U formatted string with preferences in MISC field

---

## üîç Detection Examples

### Example 1: Nominal Domain Preference

**Input:** "Ali'nin okuduƒüu kitap burada."

**Analysis:**
- `okuduƒüu`: VERB (Stanza)
- **Preference**: NOUN (90% confidence)
- **Reason**: `-DIK` suffix ‚Üí partitive predicate ‚Üí nominal domain
- **Semantic validation**: Partitive predicate in nominal position

### Example 2: Lexicalized Compound

**Input:** "Y√ºzme havuzu temiz."

**Analysis:**
- `Y√ºzme`: NOUN (Stanza)
- **Preference**: None (lexicalized)
- **Reason**: "y√ºzme havuzu" is a frozen compound (swimming pool)
- No semantic shift detected

### Example 3: Productive Derivation

**Input:** "Yazma defteri aldƒ±m."

**Analysis:**
- `Yazma`: VERB (Stanza)
- **Preference**: NOUN (85% confidence)
- **Reason**: `-mA` suffix, productive derivation
- "yazma defteri" = notebook (compositional meaning)

### Example 4: Generic vs Specific

**Input 1:** "Ku≈ülar u√ßar." (Birds fly)
- **Proposition**: Analytic
- **Predicate**: Holistic
- **Generic**: True
- **Verifiability**: 1.0 (always true)

**Input 2:** "Ku≈ülar u√ßtu." (Birds flew)
- **Proposition**: Synthetic
- **Predicate**: Partitive
- **Generic**: False
- **Time-bound**: True
- **Verifiability**: 0.8 (context-dependent)

---

## üõ†Ô∏è Advanced Configuration

### Custom Stanza Pipeline

```python
import stanza
from api.pos_semantic_analyzer import analyze_text

# Custom pipeline (advanced users)
nlp = stanza.Pipeline('tr', 
    processors='tokenize,pos,lemma,depparse',
    tokenize_pretokenized=True  # If pre-tokenized
)

# Use default pipeline
result = analyze_text("Ali geldi.")
```

### Disable Semantics

```python
# Only POS preferences, no semantics
result = analyze_text("Ku≈ülar u√ßar.", include_semantics=False)
```

---

## üìö Theoretical References

### Minimalist Program
- Chomsky, N. (1995). *The Minimalist Program*. MIT Press.
- Kornfilt, J. (1997). *Turkish*. Routledge.

### Propositional Semantics
- Carlson, G. N. (1977). *Reference to Kinds in English*. UMass dissertation.
- Chierchia, G. (1998). Reference to kinds across languages. *Natural Language Semantics* 6.

### Turkish Linguistics
- G√∂ksel, A., & Kerslake, C. (2005). *Turkish: A Comprehensive Grammar*. Routledge.
- Kornfilt, J. (1997). *Turkish*. Routledge Descriptive Grammars.

### UD Turkish
- Universal Dependencies Turkish-IMST corpus
- [UD Turkish Documentation](https://universaldependencies.org/tr/index.html)

---

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

1. **Morphological Analysis**: Better Turkish morphology extraction
2. **Semantic Features**: Expand propositional analysis
3. **Error Detection**: More nominal suffix patterns
4. **Performance**: Optimize Stanza integration
5. **Testing**: More edge cases and corpus evaluation

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Stanza NLP**: Stanford NLP Group
- **Universal Dependencies**: UD Turkish-IMST corpus
- **Theoretical foundations**: Chomsky's Minimalist Program, Carlson's Generic Reference

---

## üìß Contact

For questions or feedback, please open an issue on GitHub.

---

## üóÇÔ∏è Project History

### Recent Updates (February 2025)

- ‚úÖ **Comprehensive test suite**: test_pos_fixes.py with 17 tests (100% passing)
- ‚úÖ **PyTorch 2.6 compatibility**: Added workaround for Stanza models
- ‚úÖ **Renamed flagship API**: `structured_output.py` ‚Üí `pos_semantic_analyzer.py`
- ‚úÖ **English output**: All semantic fields now in English (holistic/partitive/habitual)
- ‚úÖ **Fixed imports**: Resolved module path issues in api/main.py
- ‚úÖ **Aspect=Hab support**: Habitual verbs now correctly detected as finite
- ‚úÖ **Type safety**: Added getattr for Stanza document access
- ‚úÖ **Project cleanup**: Removed centering theory module (18 files deleted)
- ‚úÖ **Test reorganization**: All tests moved to `tests/` directory
- ‚úÖ **Import fixes**: Updated all import paths
- ‚úÖ **Lexicalized filtering**: Improved compound detection
- ‚úÖ **Semantic validation**: 90% ‚Üí 95% confidence boost for -DIK
- ‚úÖ **Propositional semantics**: Full integration with POS analysis

### Core Features

1. **POS Preference Detection** (Minimalist Program)
   - Nominal suffix detection (-DIK, -mA, -I≈ü, -mAk)
   - Confidence scoring with semantic validation
   - Lexicalized compound filtering

2. **Propositional Semantics**
   - Analytic vs Synthetic propositions
   - Holistic vs Partitive predicates
   - Generic encoding detection
   - Clause finiteness analysis

3. **Output Formats**
   - JSON (Stanza-compatible with extensions)
   - CONLL-U (standard format with MISC annotations)

4. **Testing Suite**
   - Comprehensive integration tests
   - Semantic validation tests
   - Lexicalized compound tests
   - Minimalist Program validation

---

**Built with ‚ù§Ô∏è for Turkish NLP**
